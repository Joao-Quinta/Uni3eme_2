{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : k-Nearest Neighbor (k-NN) and linear transformations - OBLIGATORY\n",
    "\n",
    "In this TP you will implement a simple nearest Neighbors classifier using Python. \n",
    "\n",
    "First you will implement euclidean distance in three different ways (2 loops, 1 loop, without any loop) to see and understand the importance of the vectorization. Then you will train the kNN algorithm for a number of neighbors k, to check and comment on the model complexity. \n",
    "You will also implement the Mahalanobis distance and comment on the effect of a transformation on this distance.\n",
    "\n",
    "The k-NN classifier consists of two stages:  \n",
    "- During training, the classifier takes the training data and simply remembers it\n",
    "- During testing, k-NN classifies every test sample by comparing to all training samples and transfering the labels of the k most similar training examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_IRIS, load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use a dataset named CIFAR10. This dataset contains 60,000 32x32 color images in 10 different classes ([CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10)) and it will allow us to compare the three different implementations of Euclidean distance.\n",
    "\n",
    "**You have to download the dataset; open a terminal and go to the folder *datasets*, then execute the script *get_datasets.sh*:**\n",
    "```bash\n",
    "$ ./get_datasets.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise\n",
    "num_training = 5000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 500\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we can focus on k-NN precisely. We will start by implementing the Euclidean distance with two loops and check that the implementation is correct before moving forward.\n",
    "\n",
    "The k-NN algorithm can be broken down into two steps:\n",
    "- The first step is to calculate the distances between each sample of the test set and those of the training set. This distances will be stored in a matrix of shape (number of test samples, number of training samples) where each element (i,j) is the distance between the i-th test and j-th train example.\n",
    "- The second is to use this distance matrix to find the closest neighbors of each individual in the test set.\n",
    "\n",
    "Let's start with implementation :  \n",
    "Go to `distances.py` and implement the function `compute_euclidean_dist_two_loops`. This function uses a very inefficient double loop to compute the distance matrix one element at time.\n",
    "\n",
    "\n",
    "Recall:  \n",
    "The euclidean distance between two learning instances $x_i \\in R^d$ and $x_j \\in R^d$, where $d$ is the feature (attribute) dimention, is simply defined as:\n",
    "$d_2(x_i, x_j) = \\sqrt{(x_i-x_j)^T(x_i - x_j)} = \\sum \\sqrt{(x_i-x_j)^2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open distances.py and implement compute_euclidean_dist_two_loops.\n",
    "from distances import compute_euclidean_dist_two_loops\n",
    "\n",
    "# Test your implementation, this should output (500,5000)\n",
    "dists = compute_euclidean_dist_two_loops(X_train, X_test)\n",
    "print(dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that your implementation is correct implement the prediction step.\n",
    "\n",
    "Go to `k_nearest_neighbor.py` and implement the `predict_labels` method.\n",
    "\n",
    "You should expect to see approximately `27%` accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_nearest_neighbor import KNearestNeighbor\n",
    "\n",
    "# Create a k-NN classifier instance. \n",
    "# Remember that training a k-NN classifier is a noop: \n",
    "# the Classifier simply remembers the data and does no further processing \n",
    "knn = KNearestNeighbor()\n",
    "knn.train(X_train, y_train)\n",
    "\n",
    "# Don't forget to implement the following method in k_nearest_neighbor.py\n",
    "# We use k = 1 (which is the Nearest Neighbor).\n",
    "y_test_pred = knn.predict_labels(dists, k=1)\n",
    "\n",
    "# Compute and print the fraction of correctly predicted examples\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = (float(num_correct) / num_test)*100\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try out a larger `k`, say `k = 5`. You should expect a slightly better performance than with `k = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = knn.predict_labels(dists, k=5)\n",
    "num_correct = np.sum(y_test_pred == y_test)\n",
    "accuracy = (float(num_correct) / num_test)*100\n",
    "print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your implementation gives you the accuracy expected, well done !\n",
    "\n",
    "Now lets improve the performance of the algorithm ! \n",
    "\n",
    "We ask you to implement the function `compute_euclidean_dist_one_loop` in `distances.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open distances.py and implement compute_euclidean_dist_two_loops.\n",
    "from distances import compute_euclidean_dist_one_loop\n",
    "\n",
    "dists_one = compute_euclidean_dist_one_loop(X_train, X_test)\n",
    "\n",
    "# To ensure that your new implementation is correct, the test below will compare it to the naive one using the norm.\n",
    "# You should expect a value close to 0.\n",
    "\n",
    "difference = np.linalg.norm(dists - dists_one)\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation should improve the computational time, but we can do better!\n",
    "\n",
    "To make the code really fast you have to avoid the use of loops. To achieve this you should vectorize the code using the matrix operations that numpy provide you.\n",
    "\n",
    "Implement the missing part of the function `compute_euclidean_dist_no_loops` in `distances.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open distances.py and implement compute_euclidean_dist_no_loops.\n",
    "from distances import compute_euclidean_dist_no_loops\n",
    "\n",
    "dists_no = compute_euclidean_dist_no_loops(X_train, X_test)\n",
    "\n",
    "# To ensure that your new implementation is correct, the test below will compare it to the naive one using the norm.\n",
    "# You should expect a value close to 0.\n",
    "\n",
    "difference = np.linalg.norm(dists - dists_no)\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare how fast the implementations are\n",
    "def time_function(f, *args):\n",
    "    \"\"\"\n",
    "    Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    f(*args)\n",
    "    toc = time.time()\n",
    "    return toc - tic\n",
    "\n",
    "two_loop_time = time_function(compute_euclidean_dist_two_loops, X_train, X_test)\n",
    "print('Two loop version took %f seconds' % two_loop_time)\n",
    "\n",
    "one_loop_time = time_function(compute_euclidean_dist_one_loop, X_train, X_test)\n",
    "print('One loop version took %f seconds' % one_loop_time)\n",
    "\n",
    "no_loop_time = time_function(compute_euclidean_dist_no_loops, X_train, X_test)\n",
    "print('No loop version took %f seconds' % no_loop_time)\n",
    "\n",
    "# you should see significantly faster performance with the fully vectorized implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset: model complexity and decision surfaces\n",
    "\n",
    "Now that we have seen the effect of loops in the code, we will see how k affects the model. To simplify this part we  introduce a new dataset named [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). This dataset has 150 samples from 3 classes, 50 from each one (Iris setosa, Iris virginica and Iris versicolor) and each sample has 4 attributes (Sepal Length, Sepal Width, Petal Length and Petal Width). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data.\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issues)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_IRIS(test=True)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "# we use all the data as training\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis distance\n",
    "\n",
    "The mahalanobis distance is parametrized by a $d \\times d$ covariance matrix $\\Sigma$, in which case the metric is defined as:  \n",
    "\n",
    "$d(x_i, x_j; \\Sigma) = \\sqrt{(x_i-x_j)^T\\Sigma^{-1}(x_i - x_j)}$  \n",
    "\n",
    "where we can see that the euclidean distance is simply the mahalanobis distance with the identity matrix I as covariance matrix.\n",
    "\n",
    "Implement the missing part in `compute_mahalanobis_dist` in `distances.py`. You are free to implement it the way you prefer (1 for loop, 2 for loops, or without for loop).\n",
    "\n",
    "Now we will compute the Machalanobis distance using the identity covariance matrix and we will compare it with the result we get using the Euclidean distance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open distances.py and implement compute_mahalanobis_dist.\n",
    "from distances import compute_mahalanobis_dist, compute_euclidean_dist_one_loop\n",
    "from distances import define_covariance\n",
    "\n",
    "\n",
    "# euclidean distance for testing\n",
    "dists = compute_euclidean_dist_one_loop(X_train, X_test)\n",
    "\n",
    "# mahalanobis distance\n",
    "d = X_train.shape[1]\n",
    "identity = np.identity(d)\n",
    "dists_maha = compute_mahalanobis_dist(X_train, X_test, identity)\n",
    "\n",
    "# To ensure that your new implementation is correct, the test below will compare it to the naive one using the norm.\n",
    "# You should expect a value close to 0.\n",
    "\n",
    "difference = np.linalg.norm(dists - dists_maha)\n",
    "print('Difference was: %f' % (difference, ))\n",
    "if difference < 0.001:\n",
    "    print('Good! The distance matrices are the same')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan distance\n",
    "Implement the missing part in `compute_manhattan_dist` in `distances.py`. You are free to implement it the way you prefer (1 for loop, 2 for loops, or without for loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open distances.py and implement compute_manhattan_dis.\n",
    "from distances import compute_manhattan_dist\n",
    "\n",
    "# manhattan distance\n",
    "d = X_train.shape[1]\n",
    "dists_manhattan = compute_manhattan_dist(X_train, X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model complexity\n",
    "Run the kNN algorithm using the iris data set with **1, 3, 5, 10, 25, 50 neighbors. For each k use the Euclidean, the Mahalanobis, and the Manhattan  distance. **\n",
    "\n",
    "In the case of the Mahalanobis distance you are going to explore three different approaches to compute it. \n",
    "\n",
    "1. Define Σ as a diagonal matrix that has at its diagonal the average varianceof the different features, i.e. all diagonal entries $Σ_{ii}$ will be the same.\n",
    "2. Define Σ as a diagonal matrix that has at its diagonal the variance of eachfeature, i.e.$σ_k$.\n",
    "3. Define Σ as the full covariance matrix between all pairs of features.\n",
    "\n",
    "\n",
    "Explain how the performance changes with respect to the values of k and the diffent distances that you use. How does the number of the neighbors influence the classification?\n",
    "\n",
    "For a fixed number of neighbors (k= 10)  comment on the differences between the distances and on the differences between the three different versions of the Mahalanobis distances. Comment how these affect the performance of the classification, when we should prefer the oneover the other, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distances import compute_manhattan_dist, compute_mahalanobis_dist, compute_euclidean_dist_one_loop\n",
    "from distances import define_covariance\n",
    "\n",
    "# train\n",
    "knn = KNearestNeighbor()\n",
    "knn.train(X_train, y_train)\n",
    "\n",
    "# test\n",
    "k_values = [1, 3, 5, 10, 25, 50]\n",
    "n = len(X_test)\n",
    "distances = ['euclidean', 'mahalanobis', 'manhattan']\n",
    "covariances = ['diag_average_cov', 'diag_cov', 'full_cov']\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Open distances.py and                                                        #\n",
    "#   - implement compute_mahalanobis_dist,compute_manhattan_dist .              #\n",
    "#   - implement define_covariance to computethe different covariance matrices  # \n",
    "#       approaches for the Machalanobis distance   \n",
    "#                                                                              #\n",
    "#                                                                              #\n",
    "# run the kNN algorithm and compute the accuracy                               # \n",
    "#     - for the different k_values and the different distances                 #\n",
    "#     - in the case of Machalanobis distance for the 3 different sigma cases   #\n",
    "#\n",
    "# For each case print the k, distance, sigma, number of corrects over the total #\n",
    "# and the test accuray (similarly as we did above)\n",
    "# (for example you have to print something like:  \n",
    "#\n",
    "#        mahalanobis distance, cov = diag_cov:                                         #\n",
    "#        For k=10 Got 49 / 50 correct => accuracy: 0.98                           #\n",
    "################################################################################\n",
    "# Your code\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision surface\n",
    "Visualize, study, and discuss the decision surfaces that kNN algorithm produces for the different values of k using the euclidean distance.\n",
    "\n",
    "\n",
    "* To do so, you will work only in two attributes.\n",
    "\n",
    "     * Testing will be done on an artificially generated dataset that covers in a regular manner all possible values for the two chosen attributes. To do so we need to divide the space into a grid by discretizing the space  into $n$ values between the minimum and maximum value of an attribute. Each of these values must be compared with the $n$ discrete values of the second attribute. The resulting array will be of shape $(n*n, 2)$\n",
    "     \n",
    "     * Using your training set classify your test instances and visualize the results of the classification\n",
    "     \n",
    "See how the performance changes with respect to the value of k. Explain how the different values of the k change the classification boundaries in the instance space for a given training set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data.\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issues)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train = load_IRIS(test=False)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "# we use all the data as training\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_labels = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "# For our visualisation we will only keep the two first attributs.\n",
    "# You can choose 2 attribus here\n",
    "attribut_1 = 1\n",
    "attribut_2 = 3\n",
    "\n",
    "X_train_2_features = np.array([X_train[:,attribut_1], X_train[:,attribut_2]]).T\n",
    "axes_label_1 = axes_labels[attribut_1]\n",
    "axes_label_2 = axes_labels[attribut_2]\n",
    "\n",
    "# and the test set will be the matrix (n*n, 2) where n is the number of values taken between\n",
    "# the min and the max of each attribute\n",
    "n = 150\n",
    "\n",
    "X_test = np.zeros((n*n, 2))\n",
    "tmp = np.linspace(np.min(X_train_2_features[:, 0])-0.1, np.max(X_train_2_features[:, 0])+0.1, n)\n",
    "X_test[:,0] = np.repeat(tmp, n)\n",
    "\n",
    "tmp = np.linspace(np.min(X_train_2_features[:, 1])-0.1, np.max(X_train_2_features[:, 1])+0.1, n)\n",
    "X_test[:,1] = np.tile(tmp, n)\n",
    "\n",
    "\n",
    "print('Training data shape: ', X_train_2_features.shape)\n",
    "print('Testing data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "knn = KNearestNeighbor()\n",
    "knn.train(X_train_2_features, y_train)\n",
    "\n",
    "# test\n",
    "k_values = [1, 3, 5, 10, 25, 50]\n",
    "y_predicted_for_each_k = np.zeros((n*n ,len(k_values)))\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    y_predicted_for_each_k[:, i] = knn.predict(X_test, k=k_values[i], distance='euclidean', num_loops=0)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "print('Predicted data shape (n, number of k values): ', y_predicted_for_each_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision surface\n",
    "colors_surfaces = ['#88E2EA', '#FFE1BB', '#D4F3CD']\n",
    "colors_points = ['blue', 'red', 'green']\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "plt.clf()\n",
    "fig = plt.figure() \n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    for n_class in range(3):\n",
    "        x_train_attri_1 = X_train_2_features[y_train == n_class, 0]\n",
    "        x_train_attri_2 = X_train_2_features[y_train == n_class, 1]\n",
    "        \n",
    "        x_test_attri_1 = X_test[y_predicted_for_each_k[:, i] == n_class, 0]\n",
    "        x_test_attri_2 = X_test[y_predicted_for_each_k[:, i] == n_class, 1]\n",
    "        \n",
    "        ax = fig.add_subplot(2,3,1+i)  \n",
    "        ax.set_title(\"K = \"+str(k_values[i]))\n",
    "        ax.scatter(x_test_attri_1, x_test_attri_2, s=15, color=colors_surfaces[n_class])\n",
    "        ax.scatter(x_train_attri_1, x_train_attri_2, s=15, color=colors_points[n_class])\n",
    "        ax.set_xlabel(axes_label_1)\n",
    "        ax.set_ylabel(axes_label_2)\n",
    "\n",
    "        \n",
    "# each color represent a class of the problem\n",
    "\n",
    "# don't take care of the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear transformations\n",
    "\n",
    "In this exercise we are interested in the effect of a linear transformation on distances. You will need to generate points to form a unit circle and draw it. Then you have to transform your set of points with a given matrix A.\n",
    "You then have to redraw the unit circle with the points obtained and comment what you see.\n",
    "Finally you have to find the points such that after transformation we have the same points as those generated for the unit circle. The goal here is  to visualize that even a small transformation puts you in a new space where the distances are different.\n",
    "\n",
    "Explain the result of each transformation that you apply.\n",
    "\n",
    "\n",
    "### Unit circle\n",
    "Remember that the unit cicle is given from the equation $\\,  x^2 + y^2 = 1$ and in a matrix notation is written as $\\, X^TX = 1$\n",
    "\n",
    "Coding tip: you create the x variable, then y is defined from x. This means that for each x, the corresponding y value is calculated (and thus y has the same shape as x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Open transformation.py and implement unit_circle_points and draw.\n",
    "from transformation import setup_plot, draw, unit_circle_points\n",
    "\n",
    "# get axes for the plots\n",
    "fig, ax = setup_plot()\n",
    "\n",
    "# number of points to divide the space\n",
    "n = 400\n",
    "# get the points of the unit circle\n",
    "# X: A numpy array of shape (2, 2*n) where \n",
    "# X[0, :] is the set of points for the first coordinate x1 and \n",
    "# X[1, :] is the set of points for the second coordinate x2.\n",
    "\n",
    "X = unit_circle_points(n)\n",
    "\n",
    "# We print out the size of X, is it equal to (2, 800) ? \n",
    "print(\"Shape of X :\", X.shape)\n",
    "\n",
    "# draw the unit circle\n",
    "draw(ax, X, n, 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open transformation.py and implement apply_transformation_AX.\n",
    "from transformation import apply_transformation_AX\n",
    "\n",
    "fig, ax = setup_plot()\n",
    "\n",
    "# redraw the unit circle\n",
    "draw(ax, X, n, 'green')\n",
    "\n",
    "# define the transformation matrix\n",
    "A = np.array([[0.8, 0.3], [0.3, 0.8]])\n",
    "\n",
    "\n",
    "\n",
    "# apply the transformation\n",
    "AX = apply_transformation_AX(X, A)\n",
    "\n",
    "\n",
    "# We print out the size of AX, is it equal to (2, 800) ? \n",
    "print(\"Shape of AX :\", AX.shape)\n",
    "\n",
    "# draw the unit circle after transformation\n",
    "draw(ax, AX, n, 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First transformation\n",
    "explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open transformation.py and implement apply_transformation_Z.\n",
    "from transformation import apply_transformation_Z\n",
    "\n",
    "fig, ax = setup_plot()\n",
    "\n",
    "# redraw the unit circle\n",
    "draw(ax, X, n, 'green')\n",
    "# redraw the transformed circle\n",
    "draw(ax, AX, n, 'blue')\n",
    "\n",
    "# apply the transformation\n",
    "Z = apply_transformation_Z(X, A)\n",
    "\n",
    "# We print out the size of Z, is it equal to (2, 800) ? \n",
    "print(\"Shape of Z :\", Z.shape)\n",
    "\n",
    "# draw the unit circle after transformation with condition\n",
    "draw(ax, Z, n, 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
