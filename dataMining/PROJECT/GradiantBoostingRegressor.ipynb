{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f1d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from utils import get_smiles_encodings, load_data, smile_to_hot\n",
    "from model_regression import LinearRegression_RidgeRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "'''https://www.youtube.com/watch?v=0ARLObJJ3y4&t=661s'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f710d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X - train :  (92428,)\n",
      "y - train :  (92428, 4)\n",
      "X_hot :  (92428, 14)\n",
      "X_hot_test :  (39612, 14)\n",
      "y_train [[  0.6361    0.       16.043     0.     ]\n",
      " [  0.162     0.       17.031     0.     ]\n",
      " [ -0.8247    0.       18.015     0.     ]\n",
      " ...\n",
      " [ -0.5388    0.      130.143     1.     ]\n",
      " [ -1.7719    0.      131.131     1.     ]\n",
      " [ -0.89423   0.      131.131     1.     ]]\n",
      "y_test [[ -1.3449   0.     132.115    1.    ]\n",
      " [  1.9317   0.     124.183    1.    ]\n",
      " [  1.0462   0.     125.171    1.    ]\n",
      " ...\n",
      " [ -0.8808   0.     120.155    8.    ]\n",
      " [  0.3187   0.     119.167    8.    ]\n",
      " [ -0.55     0.     121.139    8.    ]]\n"
     ]
    }
   ],
   "source": [
    "file_smiles = './dataset/QM9.txt'\n",
    "file_properties = './dataset/properties_QM9.npz'\n",
    "smiles, alphabet, largest_molecule_len = get_smiles_encodings(file_smiles)\n",
    "properties = np.load(file_properties)['properties'].astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(smiles, properties)\n",
    "print(\"X - train : \", X_train.shape)\n",
    "print(\"y - train : \", y_train.shape)\n",
    "X_hot = np.array([smile_to_hot(x, largest_molecule_len, alphabet)[1].sum(axis=0) for x in X_train])\n",
    "#print(\"X_hot\", X_hot)\n",
    "X_hot_test = np.array([smile_to_hot(x, largest_molecule_len, alphabet)[1].sum(axis=0) for x in X_test])\n",
    "#print(\"X_hot_test\", X_hot_test)\n",
    "print(\"X_hot : \", X_hot.shape)\n",
    "print(\"X_hot_test : \", X_hot_test.shape)\n",
    "print(\"y_train\", y_train)\n",
    "print(\"y_test\",y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c71a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_0 = y_test[:,0]\n",
    "y_test_1 = y_test[:,1]\n",
    "y_test_2 = y_test[:,2]\n",
    "y_test_3 = y_test[:,3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "744dd222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls   10    - done\n",
      "ls   1    - done\n",
      "ls   0.1    - done\n",
      "ls   0.01    - done\n",
      "ls   0.001    - done\n",
      "ls   0.0001    - done\n",
      "ls   1e-05    - done\n",
      "lad   10    - done\n",
      "lad   1    - done\n",
      "lad   0.1    - done\n",
      "lad   0.01    - done\n",
      "lad   0.001    - done\n",
      "lad   0.0001    - done\n",
      "lad   1e-05    - done\n",
      "huber   10    - done\n",
      "huber   1    - done\n",
      "huber   0.1    - done\n",
      "huber   0.01    - done\n",
      "huber   0.001    - done\n",
      "huber   0.0001    - done\n",
      "huber   1e-05    - done\n",
      "quantile   10    - done\n",
      "quantile   1    - done\n",
      "quantile   0.1    - done\n",
      "quantile   0.01    - done\n",
      "quantile   0.001    - done\n",
      "quantile   0.0001    - done\n",
      "quantile   1e-05    - done\n",
      "\n",
      "[0.26763972449390216, 'lad', 1]\n",
      "[58.43178834696557, 'ls', 1]\n",
      "[0.6562325422345359, 'ls', 1]\n",
      "[5.399878824598602, 'huber', 1]\n"
     ]
    }
   ],
   "source": [
    "loss_function = [\"ls\", \"lad\", \"huber\", \"quantile\"]\n",
    "learning_rate_ = [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "best_0 = [100,0,0]\n",
    "best_1 = [100,0,0]\n",
    "best_2 = [100,0,0]\n",
    "best_3 = [100,0,0]\n",
    "\n",
    "#we iterate over loss function and learning rates\n",
    "for loss in loss_function:\n",
    "    for lr in learning_rate_:\n",
    "        # 0 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss)\n",
    "        reg.fit(X_hot,  y_train[:,0])\n",
    "        y_pred_0 = reg.predict(X_hot_test)\n",
    "        \n",
    "        # 1\n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss)\n",
    "        reg.fit(X_hot,  y_train[:,1])\n",
    "        y_pred_1 = reg.predict(X_hot_test)\n",
    "        y_pred_1 = np.round(y_pred_1)\n",
    "\n",
    "        # 2 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss)\n",
    "        reg.fit(X_hot,  y_train[:,2])\n",
    "        y_pred_2 = reg.predict(X_hot_test)\n",
    "        \n",
    "        # 3 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss)\n",
    "        reg.fit(X_hot,  y_train[:,3])\n",
    "        y_pred_3 = reg.predict(X_hot_test)\n",
    "        y_pred_3 = np.round(y_pred_3)\n",
    "        \n",
    "        # calcul res\n",
    "        sum_0 = 0\n",
    "        sum_1 = 0\n",
    "        sum_2 = 0\n",
    "        sum_3 = 0\n",
    "        for i in range(y_test_0.shape[0]):\n",
    "\n",
    "            # 0\n",
    "            sum_0 = sum_0 + np.abs(np.abs(y_pred_0[i]) - np.abs(y_test_0[i]))\n",
    "\n",
    "            # 1\n",
    "            if(y_pred_1[i] == y_test_1[i]):\n",
    "                sum_1 = sum_1 + 1\n",
    "\n",
    "            # 2\n",
    "            sum_2 = sum_2 + np.abs(np.abs(y_pred_2[i]) - np.abs(y_test_2[i]))\n",
    "\n",
    "            # 3\n",
    "            if(y_pred_3[i] == y_test_3[i]):\n",
    "                sum_3 = sum_3 + 1\n",
    "\n",
    "        sum_0 = sum_0 / y_test_0.shape[0]\n",
    "        sum_1_r = 100 - (sum_1 / y_test_1.shape[0] * 100)\n",
    "        sum_2 = sum_2 / y_test_2.shape[0]\n",
    "        sum_3_r = 100 - (sum_3 / y_test_3.shape[0] * 100)\n",
    "        \n",
    "        \n",
    "        # change res if needed\n",
    "        if sum_0 < best_0[0]:\n",
    "            best_0 = [sum_0, loss, lr]\n",
    "\n",
    "        if sum_1_r < best_1[0]:\n",
    "            best_1 = [sum_1_r, loss, lr]\n",
    "            \n",
    "        if sum_2 < best_2[0]:\n",
    "            best_2 = [sum_2, loss, lr]\n",
    "\n",
    "        if sum_3_r < best_3[0]:\n",
    "            best_3 = [sum_3_r, loss, lr]\n",
    "\n",
    "        print(loss, \" \", lr, \"   - done\")\n",
    "        \n",
    "print(\"OPTIMAL RESULTS\")                \n",
    "print(best_0)\n",
    "print(best_1)\n",
    "print(best_2)\n",
    "print(best_3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "234d69db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS DEF : Loss function to be optimized. ‘ls’ refers to least squares regression. ‘lad’ (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).\n",
      "\n",
      "logP: represents a measure of the tendency of a compound to move from the aqueous phase into lipids\n",
      "\n",
      "model -> loss_function :  lad  - learning rate :  1\n",
      "\n",
      "Error is on avearage :  0.26763972449390216\n",
      "Let's say the target value is : 1.0, then the model would guess a value in the following interval :\n",
      " -> [ 0.7323602755060978 , 1.2676397244939022 ]\n"
     ]
    }
   ],
   "source": [
    "properties_txt = ['logP: represents a measure of the tendency of a compound to move from the aqueous phase into lipids',\n",
    "                  'Number of rotatable bonds (RBN): the number of bonds which allow free rotation around themselves',\n",
    "                  'Molecular weight (MW): the weight of a molecule based on the atomic masses of all atoms in the molecule',\n",
    "                  'Number of the rings (RN): the number of connected sets of atoms and bonds in which every atom and bond is a member of a cycle']\n",
    "\n",
    "print(\"LOSS DEF : Loss function to be optimized. ‘ls’ refers to least squares regression. ‘lad’ (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(properties_txt[0])\n",
    "print()\n",
    "print(\"model -> loss_function : \", best_0[1], \" - learning rate : \", best_0[2])\n",
    "print()\n",
    "print(\"Error is on avearage : \", best_0[0])\n",
    "print(\"Let's say the target value is : 1.0, then the model would guess a value in the following interval :\")\n",
    "print(\" -> [\", 1.0 - best_0[0],\",\",1.0 + best_0[0],\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfd208df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rotatable bonds (RBN): the number of bonds which allow free rotation around themselves\n",
      "\n",
      "model -> max_iter :  ls  - learning rate :  1\n",
      "\n",
      "Error rate is :  58.43178834696557 %\n"
     ]
    }
   ],
   "source": [
    "print(properties_txt[1])\n",
    "print()\n",
    "print(\"model -> loss_function : \", best_1[1], \" - learning rate : \", best_1[2])\n",
    "print()\n",
    "print(\"Error rate is : \", best_1[0], \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556a7039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecular weight (MW): the weight of a molecule based on the atomic masses of all atoms in the molecule\n",
      "\n",
      "model -> loss_function :  ls  - learning rate :  1\n",
      "\n",
      "Error is on avearage :  0.6562325422345359\n",
      "Let's say the target value is : 150, then the model would guess a value in the following interval :\n",
      " -> [ 149.34376745776547 , 150.65623254223453 ]\n"
     ]
    }
   ],
   "source": [
    "print(properties_txt[2])\n",
    "print()\n",
    "print(\"model -> loss_function : \", best_2[1], \" - learning rate : \", best_2[2])\n",
    "print()\n",
    "print(\"Error is on avearage : \", best_2[0])\n",
    "print(\"Let's say the target value is : 150, then the model would guess a value in the following interval :\")\n",
    "print(\" -> [\", 150.0 - best_2[0],\",\",150.0 + best_2[0],\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15924bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the rings (RN): the number of connected sets of atoms and bonds in which every atom and bond is a member of a cycle\n",
      "\n",
      "model -> loss_function :  huber  - learning rate :  1\n",
      "\n",
      "Error rate is :  5.399878824598602 %\n"
     ]
    }
   ],
   "source": [
    "print(properties_txt[3])\n",
    "print()\n",
    "print(\"model -> loss_function : \", best_3[1], \" - learning rate : \", best_3[2])\n",
    "print()\n",
    "print(\"Error rate is : \", best_3[0], \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fd3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3dd1f1",
   "metadata": {},
   "source": [
    "## comment\n",
    "Since learning rate has no impact, we changed it for the max_depth parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d913bf",
   "metadata": {},
   "source": [
    "## Tests without learning rate (=1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5dbc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls   3    - done\n",
      "ls   5    - done\n",
      "ls   7    - done\n",
      "ls   9    - done\n",
      "lad   3    - done\n",
      "lad   5    - done\n",
      "lad   7    - done\n",
      "lad   9    - done\n",
      "huber   3    - done\n",
      "huber   5    - done\n",
      "huber   7    - done\n",
      "huber   9    - done\n",
      "quantile   3    - done\n",
      "quantile   5    - done\n",
      "quantile   7    - done\n",
      "quantile   9    - done\n",
      "\n",
      "[0.26763972449390216, 'lad', 3]\n",
      "[58.43178834696557, 'ls', 3]\n",
      "[0.6153660201806783, 'huber', 7]\n",
      "[5.399878824598602, 'huber', 3]\n"
     ]
    }
   ],
   "source": [
    "loss_function = [\"ls\", \"lad\", \"huber\", \"quantile\"]\n",
    "liste_max_depth = [3, 5, 7, 9]\n",
    "best_0 = [100,0,0]\n",
    "best_1 = [100,0,0]\n",
    "best_2 = [100,0,0]\n",
    "best_3 = [100,0,0]\n",
    "\n",
    "#we iterate over loss function and learning rates\n",
    "for loss in loss_function:\n",
    "    for max_depth in liste_max_depth:\n",
    "        # 0 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=1, loss=loss, max_depth=max_depth)\n",
    "        reg.fit(X_hot,  y_train[:,0])\n",
    "        y_pred_0 = reg.predict(X_hot_test)\n",
    "        \n",
    "        # 1\n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=1, loss=loss, max_depth=max_depth)\n",
    "        reg.fit(X_hot,  y_train[:,1])\n",
    "        y_pred_1 = reg.predict(X_hot_test)\n",
    "        y_pred_1 = np.round(y_pred_1)\n",
    "\n",
    "        # 2 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=1, loss=loss, max_depth=max_depth)\n",
    "        reg.fit(X_hot,  y_train[:,2])\n",
    "        y_pred_2 = reg.predict(X_hot_test)\n",
    "        \n",
    "        # 3 \n",
    "        reg = GradientBoostingRegressor(random_state=0, learning_rate=1, loss=loss, max_depth=max_depth)\n",
    "        reg.fit(X_hot,  y_train[:,3])\n",
    "        y_pred_3 = reg.predict(X_hot_test)\n",
    "        y_pred_3 = np.round(y_pred_3)\n",
    "        \n",
    "        # calcul res\n",
    "        sum_0 = 0\n",
    "        sum_1 = 0\n",
    "        sum_2 = 0\n",
    "        sum_3 = 0\n",
    "        for i in range(y_test_0.shape[0]):\n",
    "\n",
    "            # 0\n",
    "            sum_0 = sum_0 + np.abs(np.abs(y_pred_0[i]) - np.abs(y_test_0[i]))\n",
    "\n",
    "            # 1\n",
    "            if(y_pred_1[i] == y_test_1[i]):\n",
    "                sum_1 = sum_1 + 1\n",
    "\n",
    "            # 2\n",
    "            sum_2 = sum_2 + np.abs(np.abs(y_pred_2[i]) - np.abs(y_test_2[i]))\n",
    "\n",
    "            # 3\n",
    "            if(y_pred_3[i] == y_test_3[i]):\n",
    "                sum_3 = sum_3 + 1\n",
    "\n",
    "        sum_0 = sum_0 / y_test_0.shape[0]\n",
    "        sum_1_r = 100 - (sum_1 / y_test_1.shape[0] * 100)\n",
    "        sum_2 = sum_2 / y_test_2.shape[0]\n",
    "        sum_3_r = 100 - (sum_3 / y_test_3.shape[0] * 100)\n",
    "        \n",
    "        \n",
    "        # change res if needed\n",
    "        if sum_0 < best_0[0]:\n",
    "            best_0 = [sum_0, loss, max_depth]\n",
    "\n",
    "        if sum_1_r < best_1[0]:\n",
    "            best_1 = [sum_1_r, loss, max_depth]\n",
    "            \n",
    "        if sum_2 < best_2[0]:\n",
    "            best_2 = [sum_2, loss, max_depth]\n",
    "\n",
    "        if sum_3_r < best_3[0]:\n",
    "            best_3 = [sum_3_r, loss, max_depth]\n",
    "\n",
    "        print(loss, \" \", max_depth, \"   - done\")\n",
    "        \n",
    "print()                \n",
    "print(best_0)\n",
    "print(best_1)\n",
    "print(best_2)\n",
    "print(best_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e3284",
   "metadata": {},
   "source": [
    "## Tests WITH learning rate AND max_depth :\n",
    "We still want to be sure the learning rate dosen't have an impact when changing the max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21fc24fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls   10   3    - done\n",
      "ls   10   4    - done\n",
      "ls   1   3    - done\n",
      "ls   1   4    - done\n",
      "ls   0.1   3    - done\n",
      "ls   0.1   4    - done\n",
      "ls   0.01   3    - done\n",
      "ls   0.01   4    - done\n",
      "ls   0.001   3    - done\n",
      "ls   0.001   4    - done\n",
      "ls   0.0001   3    - done\n",
      "ls   0.0001   4    - done\n",
      "lad   10   3    - done\n",
      "lad   10   4    - done\n",
      "lad   1   3    - done\n",
      "lad   1   4    - done\n",
      "lad   0.1   3    - done\n",
      "lad   0.1   4    - done\n",
      "lad   0.01   3    - done\n",
      "lad   0.01   4    - done\n",
      "lad   0.001   3    - done\n",
      "lad   0.001   4    - done\n",
      "lad   0.0001   3    - done\n",
      "lad   0.0001   4    - done\n",
      "huber   10   3    - done\n",
      "huber   10   4    - done\n",
      "huber   1   3    - done\n",
      "huber   1   4    - done\n",
      "huber   0.1   3    - done\n",
      "huber   0.1   4    - done\n",
      "huber   0.01   3    - done\n",
      "huber   0.01   4    - done\n",
      "huber   0.001   3    - done\n",
      "huber   0.001   4    - done\n",
      "huber   0.0001   3    - done\n",
      "huber   0.0001   4    - done\n",
      "quantile   10   3    - done\n",
      "quantile   10   4    - done\n",
      "quantile   1   3    - done\n",
      "quantile   1   4    - done\n",
      "quantile   0.1   3    - done\n",
      "quantile   0.1   4    - done\n",
      "quantile   0.01   3    - done\n",
      "quantile   0.01   4    - done\n",
      "quantile   0.001   3    - done\n",
      "quantile   0.001   4    - done\n",
      "quantile   0.0001   3    - done\n",
      "quantile   0.0001   4    - done\n",
      "\n",
      "[0.26763972449390216, 'lad', 1, 3]\n",
      "[58.361102696152685, 'ls', 1, 4]\n",
      "[0.6562325422345359, 'ls', 1, 3]\n",
      "[5.399878824598602, 'huber', 1, 3]\n"
     ]
    }
   ],
   "source": [
    "loss_function = [\"ls\", \"lad\", \"huber\", \"quantile\"]\n",
    "learning_rate_ = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "liste_max_depth = [3, 4]\n",
    "best_0 = [100,0,0,0]\n",
    "best_1 = [100,0,0,0]\n",
    "best_2 = [100,0,0,0]\n",
    "best_3 = [100,0,0,0]\n",
    "\n",
    "#we iterate over loss function and learning rates\n",
    "for loss in loss_function:\n",
    "    for lr in learning_rate_:\n",
    "        for max_depth in liste_max_depth:\n",
    "\n",
    "            # 0 \n",
    "            reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss, max_depth=max_depth)\n",
    "            reg.fit(X_hot,  y_train[:,0])\n",
    "            y_pred_0 = reg.predict(X_hot_test)\n",
    "\n",
    "            # 1\n",
    "            reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss, max_depth=max_depth)\n",
    "            reg.fit(X_hot,  y_train[:,1])\n",
    "            y_pred_1 = reg.predict(X_hot_test)\n",
    "            y_pred_1 = np.round(y_pred_1)\n",
    "\n",
    "            # 2 \n",
    "            reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss, max_depth=max_depth)\n",
    "            reg.fit(X_hot,  y_train[:,2])\n",
    "            y_pred_2 = reg.predict(X_hot_test)\n",
    "\n",
    "            # 3 \n",
    "            reg = GradientBoostingRegressor(random_state=0, learning_rate=lr, loss=loss, max_depth=max_depth)\n",
    "            reg.fit(X_hot,  y_train[:,3])\n",
    "            y_pred_3 = reg.predict(X_hot_test)\n",
    "            y_pred_3 = np.round(y_pred_3)\n",
    "\n",
    "            # calcul res\n",
    "            sum_0 = 0\n",
    "            sum_1 = 0\n",
    "            sum_2 = 0\n",
    "            sum_3 = 0\n",
    "            for i in range(y_test_0.shape[0]):\n",
    "\n",
    "                # 0\n",
    "                sum_0 = sum_0 + np.abs(np.abs(y_pred_0[i]) - np.abs(y_test_0[i]))\n",
    "\n",
    "                # 1\n",
    "                if(y_pred_1[i] == y_test_1[i]):\n",
    "                    sum_1 = sum_1 + 1\n",
    "\n",
    "                # 2\n",
    "                sum_2 = sum_2 + np.abs(np.abs(y_pred_2[i]) - np.abs(y_test_2[i]))\n",
    "\n",
    "                # 3\n",
    "                if(y_pred_3[i] == y_test_3[i]):\n",
    "                    sum_3 = sum_3 + 1\n",
    "\n",
    "            sum_0 = sum_0 / y_test_0.shape[0]\n",
    "            sum_1_r = 100 - (sum_1 / y_test_1.shape[0] * 100)\n",
    "            sum_2 = sum_2 / y_test_2.shape[0]\n",
    "            sum_3_r = 100 - (sum_3 / y_test_3.shape[0] * 100)\n",
    "\n",
    "\n",
    "            # change res if needed\n",
    "            if sum_0 < best_0[0]:\n",
    "                best_0 = [sum_0, loss, lr, max_depth]\n",
    "\n",
    "            if sum_1_r < best_1[0]:\n",
    "                best_1 = [sum_1_r, loss, lr, max_depth]\n",
    "\n",
    "            if sum_2 < best_2[0]:\n",
    "                best_2 = [sum_2, loss, lr, max_depth]\n",
    "\n",
    "            if sum_3_r < best_3[0]:\n",
    "                best_3 = [sum_3_r, loss, lr, max_depth]\n",
    "\n",
    "            print(loss, \" \", lr, \" \", max_depth, \"   - done\")\n",
    "        \n",
    "print()                \n",
    "print(best_0)\n",
    "print(best_1)\n",
    "print(best_2)\n",
    "print(best_3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70157e1",
   "metadata": {},
   "source": [
    "### comment\n",
    "We see no difference for the learning rate, but a change in the second attribut with max_depth=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca02211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_errors_0_2 {'lienar_regression': [0.34321702049967723, 0.017421293770233204], 'neural_network': [0.26404827656252755, 2.9858224688390417e-06], 'GradBoostRegressor': [0.26763972449390216, 0.6562325422345359], 'SVR': [0.26732008420568637, 0.8983270988918186]}\n",
      "dict_errors_1_3 {'GradBoostRegressor': [58.43178834696557, 5.399878824598602], 'SVR': [57.472483085933554, 7.290719983843275], 'lienar_regression': [56.280924972230636, 5.710390790669493], 'neural_network': [45.78915480157528, 35.03483792790064], 'neer_neighboor': [54.907603756437446, 7.081187518933646]}\n",
      "dict_errors, after changes :  {'lienar_regression': [0.34321702049967723, 0.017421293770233204], 'neural_network': [0.26404827656252755, 2.9858224688390417e-06], 'GradBoostRegressor': [0.26763972449390216, 0.6153660201806783], 'SVR': [0.26732008420568637, 0.8983270988918186]}\n",
      "dict_errors, after changes :  {'GradBoostRegressor': [58.361102696152685, 5.399878824598602], 'SVR': [57.472483085933554, 7.290719983843275], 'lienar_regression': [56.280924972230636, 5.710390790669493], 'neural_network': [45.78915480157528, 35.03483792790064], 'neer_neighboor': [54.907603756437446, 7.081187518933646]}\n"
     ]
    }
   ],
   "source": [
    "import json #ne jamais faire ça avec les import \n",
    "\n",
    "def load_dict_errors(path=\"all_errors.json\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        dict_errors = json.load(f)\n",
    "        \n",
    "    return dict_errors\n",
    "\n",
    "dict_errors_0_2 = load_dict_errors(path=\"all_errors_0_2.json\")\n",
    "dict_errors_1_3 = load_dict_errors(path=\"all_errors_1_3.json\")\n",
    "print(\"dict_errors_0_2\", dict_errors_0_2)\n",
    "print(\"dict_errors_1_3\", dict_errors_1_3)\n",
    "\n",
    "\n",
    "def save_dict_errors(dict_errors, path=\"all_errors.json\"):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(dict_errors, f)\n",
    "\n",
    "#dict_errors_0_2['GradBoostRegressor'] = [best_0[0], best_2[0]]\n",
    "#dict_errors_1_3['GradBoostRegressor'] = [best_1[0], best_3[0]]\n",
    "\n",
    "dict_errors_0_2['GradBoostRegressor'] = [0.26763972449390216, 0.6153660201806783]\n",
    "dict_errors_1_3['GradBoostRegressor'] = [58.361102696152685, 5.399878824598602]\n",
    "\n",
    "print(\"dict_errors, after changes : \",dict_errors_0_2)\n",
    "print(\"dict_errors, after changes : \",dict_errors_1_3)\n",
    "\n",
    "save_dict_errors(dict_errors_0_2, path=\"all_errors_0_2.json\")\n",
    "save_dict_errors(dict_errors_1_3, path=\"all_errors_1_3.json\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c703d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_0 [-0.7190045   1.88391678  1.14099148 ... -0.484946    0.50922493\n",
      " -0.40514258]\n",
      "y_pred_0 len 39612\n",
      "y_pred_1 [1.32849193 1.024067   1.05879624 ... 0.11713414 0.08986237 0.08591519]\n",
      "y_pred_1 len 39612\n"
     ]
    }
   ],
   "source": [
    "#clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_hot,  y_train[:,0])\n",
    "#clf.score(X_hot,  y_train[:,0])\n",
    "'''\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_hot,  y_train[:,0])\n",
    "#reg.predict(X_test[1:2])\n",
    "#reg.score(X_test, y_test)\n",
    "y_pred_0 = reg.predict(X_hot_test)\n",
    "print(\"y_pred_0\",y_pred_0)\n",
    "print(\"y_pred_0 len\", len(y_pred_0))\n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_hot,  y_train[:,1])\n",
    "y_pred_1 = reg.predict(X_hot_test)\n",
    "\n",
    "print(\"y_pred_1\",y_pred_1)\n",
    "print(\"y_pred_1 len\", len(y_pred_1))\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_hot,  y_train[:,2])\n",
    "y_pred_2 = reg.predict(X_hot_test)\n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_hot,  y_train[:,3])\n",
    "y_pred_3 = reg.predict(X_hot_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed94b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13933d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''y_test_0 = y_test[:,0]\n",
    "y_test_1 = y_test[:,1]\n",
    "y_test_2 = y_test[:,2]\n",
    "y_test_3 = y_test[:,3]   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5373f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in y_test[:,0] :  6400\n",
      "Min of y_test[:,0] :  -2.8142\n",
      "Max of y_test[:,0] :  3.7569\n",
      "\n",
      "Number of unique values in y_pred :  1336\n",
      "Min of y_pred :  -1.5296909604895872\n",
      "Max of y_pred :  3.1377478150989435\n",
      "\n",
      "Error is on avearage :  0.3059346708449163\n",
      "Let's say the target value is : 1.0, then the model would guess a value in the following interval :\n",
      " -> [ 0.6940653291550837 , 1.3059346708449162 ]\n",
      "\n",
      "There were  0  correct predictions, out of  39612 , which is  100.0 % error rate\n",
      "\n",
      "Number of unique values in y_pred_rounded :  6\n",
      "Min of y_pred_rounded :  -2.0\n",
      "Max of y_pred_rounded :  3.0\n",
      "\n",
      "Error is on avearage :  0.4064572520574313\n",
      "There were  2  correct predictions, out of  39612 , which is  99.99495102494194 % error rate\n"
     ]
    }
   ],
   "source": [
    " '''\n",
    "\n",
    "sum_0 = 0\n",
    "sum_0_5 = 0\n",
    "for i in range(y_test_0.shape[0]):\n",
    "    sum_0 = sum_0 + np.abs(np.abs(y_pred_0[i]) - np.abs(y_test_0[i]))\n",
    "    if(y_pred_0[i] == y_test_0[i]):\n",
    "        sum_0_5 = sum_0_5 + 1\n",
    "sum_0 = sum_0 / y_test_0.shape[0]\n",
    "sum_0_5_r = sum_0_5 / y_test_0.shape[0] * 100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of unique values in y_test[:,0] : \", np.unique(y_test_0).shape[0])\n",
    "print(\"Min of y_test[:,0] : \", np.min(y_test_0))\n",
    "print(\"Max of y_test[:,0] : \", np.max(y_test_0))\n",
    "print()\n",
    "print(\"Number of unique values in y_pred : \", np.unique(y_pred_0).shape[0])\n",
    "print(\"Min of y_pred : \", np.min(y_pred_0))\n",
    "print(\"Max of y_pred : \", np.max(y_pred_0))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_0)\n",
    "print(\"Let's say the target value is : 1.0, then the model would guess a value in the following interval :\")\n",
    "print(\" -> [\", 1.0 - sum_0,\",\",1.0 + sum_0,\"]\")\n",
    "print()\n",
    "print(\"There were \", sum_0_5, \" correct predictions, out of \", y_test_0.shape[0], \", which is \", 100 - sum_0_5_r,\"% error rate\")\n",
    "print()\n",
    "\n",
    "y_pred_0_r = np.around(y_pred_0)\n",
    "\n",
    "sum_0_r = 0\n",
    "sum_0_5_r = 0\n",
    "for i in range(y_pred_0_r.shape[0]):\n",
    "    sum_0_r = sum_0_r + np.abs(np.abs(y_pred_0_r[i]) - np.abs(y_test_0[i]))\n",
    "    if(y_pred_0_r[i] == y_test_0[i]):\n",
    "        sum_0_5_r = sum_0_5_r + 1\n",
    "sum_0_r = sum_0_r / y_test_0.shape[0]\n",
    "sum_0_5_r_ = sum_0_5_r / y_test_0.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_pred_rounded : \", np.unique(y_pred_0_r).shape[0])\n",
    "print(\"Min of y_pred_rounded : \", np.min(y_pred_0_r))\n",
    "print(\"Max of y_pred_rounded : \", np.max(y_pred_0_r))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_0_r)\n",
    "print(\"There were \", sum_0_5_r, \" correct predictions, out of \", y_test_0.shape[0], \", which is \", 100 - sum_0_5_r_,\"% error rate\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc62547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in y_test[:,1] :  7\n",
      "Min of y_test[:,1] :  0.0\n",
      "Max of y_test[:,1] :  6.0\n",
      "\n",
      "Number of unique values in y_pred :  1282\n",
      "Min of y_pred :  -0.48861950331383575\n",
      "Max of y_pred :  4.1597613162624505\n",
      "\n",
      "Error is on avearage :  0.7635997201650376\n",
      "There were  0  correct predictions, out of  39612 , which is  100.0 % error rate\n",
      "\n",
      "Since the number of unique values is small, let's round the y_pred, to see if we get a better approximation : \n",
      "\n",
      "Number of unique values in y_pred_rounded :  5\n",
      "Min of y_pred_rounded :  0.0\n",
      "Max of y_pred_rounded :  4.0\n",
      "\n",
      "Error is on avearage :  0.7966777744117944\n",
      "There were  14237  correct predictions, out of  39612 , which is  64.05887104917701 % error rate\n"
     ]
    }
   ],
   "source": [
    "'''sum_1 = 0\n",
    "sum_1_5 = 0\n",
    "for i in range(y_test_1.shape[0]):\n",
    "    sum_1 = sum_1 + np.abs(np.abs(y_pred_1[i]) - np.abs(y_test_1[i]))\n",
    "    if(y_pred_1[i] == y_test_1[i]):\n",
    "        sum_1_5 = sum_1_5 + 1\n",
    "sum_1 = sum_1 / y_test_1.shape[0]\n",
    "sum_1_5_r = sum_1_5 / y_test_1.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_test[:,1] : \", np.unique(y_test_1).shape[0])\n",
    "print(\"Min of y_test[:,1] : \", np.min(y_test_1))\n",
    "print(\"Max of y_test[:,1] : \", np.max(y_test_1))\n",
    "print()\n",
    "print(\"Number of unique values in y_pred : \", np.unique(y_pred_1).shape[0])\n",
    "print(\"Min of y_pred : \", np.min(y_pred_1))\n",
    "print(\"Max of y_pred : \", np.max(y_pred_1))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_1)\n",
    "print(\"There were \", sum_1_5, \" correct predictions, out of \", y_test_1.shape[0], \", which is \", 100 - sum_1_5_r,\"% error rate\")\n",
    "print()\n",
    "print(\"Since the number of unique values is small, let's round the y_pred, to see if we get a better approximation : \")\n",
    "print()\n",
    "\n",
    "y_pred_1_r = np.around(y_pred_1)\n",
    "\n",
    "sum_1_r = 0\n",
    "sum_1_5_r = 0\n",
    "for i in range(y_pred_1_r.shape[0]):\n",
    "    sum_1_r = sum_1_r + np.abs(np.abs(y_pred_1_r[i]) - np.abs(y_test_1[i]))\n",
    "    if(y_pred_1_r[i] == y_test_1[i]):\n",
    "        sum_1_5_r = sum_1_5_r + 1\n",
    "sum_1_r = sum_1_r / y_test_1.shape[0]\n",
    "sum_1_5_r_ = sum_1_5_r / y_test_1.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_pred_rounded : \", np.unique(y_pred_1_r).shape[0])\n",
    "print(\"Min of y_pred_rounded : \", np.min(y_pred_1_r))\n",
    "print(\"Max of y_pred_rounded : \", np.max(y_pred_1_r))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_1_r)\n",
    "print(\"There were \", sum_1_5_r, \" correct predictions, out of \", y_test_1.shape[0], \", which is \", 100 - sum_1_5_r_,\"% error rate\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7867fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in y_test[:,2] :  222\n",
      "Min of y_test[:,2] :  104.152\n",
      "Max of y_test[:,2] :  152.037\n",
      "\n",
      "Number of unique values in y_pred :  1221\n",
      "Min of y_pred :  107.78899101672297\n",
      "Max of y_pred :  133.4608887238207\n",
      "\n",
      "Error is on avearage :  3.882566534691418\n",
      "Error rate is on avearage :  3.029887968784465 % -> for each guess, the model is of by  3.029887968784465 %\n",
      "Let's say the target value is : 150, then the model would guess a value in the following interval :\n",
      " -> [ 146.11743346530858 , 153.88256653469142 ]\n",
      "\n",
      "There were  0  correct predictions, out of  39612 , which is  100.0 % error rate\n",
      "\n",
      "Number of unique values in y_pred_rounded :  25\n",
      "Min of y_pred_rounded :  108.0\n",
      "Max of y_pred_rounded :  133.0\n",
      "\n",
      "Error is on avearage :  3.907632413854698\n",
      "There were  0  correct predictions, out of  39612 , which is  100.0 % error rate\n"
     ]
    }
   ],
   "source": [
    "'''sum_2 = 0\n",
    "sum_2_5 = 0\n",
    "sum_2_r = 0\n",
    "for i in range(y_test_2.shape[0]):\n",
    "    sum_2 = sum_2 + np.abs(np.abs(y_pred_2[i]) - np.abs(y_test_2[i]))\n",
    "    sum_2_r = sum_2_r + np.abs(100 - (np.abs(y_pred_2[i]) * 100 / np.abs(y_test_2[i])))\n",
    "    if(y_pred_2[i] == y_test_2[i]):\n",
    "        sum_2_5 = sum_2_5 + 1\n",
    "sum_2 = sum_2 / y_test_2.shape[0]\n",
    "sum_2_r = sum_2_r / y_test_2.shape[0]\n",
    "sum_2_5_r = sum_2_5 / y_test_2.shape[0] * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of unique values in y_test[:,2] : \", np.unique(y_test_2).shape[0])\n",
    "print(\"Min of y_test[:,2] : \", np.min(y_test_2))\n",
    "print(\"Max of y_test[:,2] : \", np.max(y_test_2))\n",
    "print()\n",
    "print(\"Number of unique values in y_pred : \", np.unique(y_pred_2).shape[0])\n",
    "print(\"Min of y_pred : \", np.min(y_pred_2))\n",
    "print(\"Max of y_pred : \", np.max(y_pred_2))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_2)\n",
    "print(\"Error rate is on avearage : \", sum_2_r, \"% -> for each guess, the model is of by \", sum_2_r, \"%\")\n",
    "print(\"Let's say the target value is : 150, then the model would guess a value in the following interval :\")\n",
    "print(\" -> [\", 150 - sum_2,\",\",150 + sum_2,\"]\")\n",
    "print()\n",
    "print(\"There were \", sum_2_5, \" correct predictions, out of \", y_test_2.shape[0], \", which is \", 100 - sum_2_5_r,\"% error rate\")\n",
    "print()\n",
    "\n",
    "y_pred_2_r = np.around(y_pred_2)\n",
    "\n",
    "sum_2_r = 0\n",
    "sum_2_5_r = 0\n",
    "for i in range(y_pred_2_r.shape[0]):\n",
    "    sum_2_r = sum_2_r + np.abs(np.abs(y_pred_2_r[i]) - np.abs(y_test_2[i]))\n",
    "    if(y_pred_2_r[i] == y_test_2[i]):\n",
    "        sum_2_5_r = sum_2_5_r + 1\n",
    "sum_2_r = sum_2_r / y_test_2.shape[0]\n",
    "sum_2_5_r_ = sum_2_5_r / y_test_2.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_pred_rounded : \", np.unique(y_pred_2_r).shape[0])\n",
    "print(\"Min of y_pred_rounded : \", np.min(y_pred_2_r))\n",
    "print(\"Max of y_pred_rounded : \", np.max(y_pred_2_r))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_2_r)\n",
    "print(\"There were \", sum_2_5_r, \" correct predictions, out of \", y_test_2.shape[0], \", which is \", 100 - sum_2_5_r_,\"% error rate\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c520158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in y_test[:,3] :  9\n",
      "Min of y_test[:,3] :  0.0\n",
      "Max of y_test[:,3] :  8.0\n",
      "\n",
      "Number of unique values in y_pred :  650\n",
      "Min of y_pred :  -0.001379710303792963\n",
      "Max of y_pred :  6.610218580449615\n",
      "\n",
      "Error is on avearage :  0.1045783467369218\n",
      "There were  0  correct predictions, out of  39612 , which is  100.0 % error rate\n",
      "\n",
      "Since the number of unique values is small, let's round the y_pred, to see if we get a better approximation : \n",
      "\n",
      "Number of unique values in y_pred_rounded :  8\n",
      "Min of y_pred_rounded :  0.0\n",
      "Max of y_pred_rounded :  7.0\n",
      "\n",
      "Error is on avearage :  0.0725790164596587\n",
      "There were  36887  correct predictions, out of  39612 , which is  6.879228516611121 % error rate\n"
     ]
    }
   ],
   "source": [
    "'''sum_3 = 0\n",
    "sum_3_5 = 0\n",
    "for i in range(y_test_3.shape[0]):\n",
    "    sum_3 = sum_3 + np.abs(np.abs(y_pred_3[i]) - np.abs(y_test_3[i]))\n",
    "    if(y_pred_3[i] == y_test_3[i]):\n",
    "        sum_3_5 = sum_3_5 + 1\n",
    "sum_3 = sum_3 / y_test_1.shape[0]\n",
    "sum_3_5_r = sum_3_5 / y_test_1.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_test[:,3] : \", np.unique(y_test_3).shape[0])\n",
    "print(\"Min of y_test[:,3] : \", np.min(y_test_3))\n",
    "print(\"Max of y_test[:,3] : \", np.max(y_test_3))\n",
    "print()\n",
    "print(\"Number of unique values in y_pred : \", np.unique(y_pred_3).shape[0])\n",
    "print(\"Min of y_pred : \", np.min(y_pred_3))\n",
    "print(\"Max of y_pred : \", np.max(y_pred_3))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_3)\n",
    "print(\"There were \", sum_3_5, \" correct predictions, out of \", y_test_3.shape[0], \", which is \", 100 - sum_3_5_r,\"% error rate\")\n",
    "print()\n",
    "print(\"Since the number of unique values is small, let's round the y_pred, to see if we get a better approximation : \")\n",
    "print()\n",
    "\n",
    "y_pred_3_r = np.around(y_pred_3)\n",
    "\n",
    "sum_3_r = 0\n",
    "sum_3_5_r = 0\n",
    "for i in range(y_test_3.shape[0]):\n",
    "    sum_3_r = sum_3_r + np.abs(np.abs(y_pred_3_r[i]) - np.abs(y_test_3[i]))\n",
    "    if(y_pred_3_r[i] == y_test_3[i]):\n",
    "        sum_3_5_r = sum_3_5_r + 1\n",
    "sum_3_r = sum_3_r / y_test_3.shape[0]\n",
    "sum_3_5_r_ = sum_3_5_r / y_test_3.shape[0] * 100\n",
    "\n",
    "print(\"Number of unique values in y_pred_rounded : \", np.unique(y_pred_3_r).shape[0])\n",
    "print(\"Min of y_pred_rounded : \", np.min(y_pred_3_r))\n",
    "print(\"Max of y_pred_rounded : \", np.max(y_pred_3_r))\n",
    "print()\n",
    "print(\"Error is on avearage : \", sum_3_r)\n",
    "print(\"There were \", sum_3_5_r, \" correct predictions, out of \", y_test_3.shape[0], \", which is \", 100 - sum_3_5_r_,\"% error rate\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652fbaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import json #ne jamais faire ça avec les import \n",
    "\n",
    "def load_dict_errors(path=\"all_errors.json\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        dict_errors = json.load(f)\n",
    "        \n",
    "    return dict_errors\n",
    "\n",
    "dict_errors = load_dict_errors()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c40a8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def save_dict_errors(dict_errors, path=\"all_errors.json\"):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(dict_errors, f)\n",
    "\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ef4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GradBoostRegressor': [0, 64.05887104917701, 0, 6.879228516611121], 'LinearRegression': [0, 57.64162375037867, 0, 7.911743915985056], 'SVR': [0, 57.472483085933554, 0, 7.290719983843275]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#dict_errors['GradBoostRegressor'] = [100 - sum_0_5_r_, 100 - sum_1_5_r_, 100 - sum_2_5_r_, 100 - sum_3_5_r_]\n",
    "dict_errors['GradBoostRegressor'] = [0, 100 - sum_1_5_r_, 0, 100 - sum_3_5_r_]\n",
    "\n",
    "#dict_errors = { \"GradBoostRegressor\" : [100 - sum_0_5_r_, 100 - sum_1_5_r_, 100 - sum_2_5_r_, 100 - sum_3_5_r_]}\n",
    "\n",
    "print(dict_errors)\n",
    "save_dict_errors(dict_errors)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3609cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_errors_0_2 {'lienar_regression': [0.34321702049967723, 0.017421293770233204], 'neural_network': [0.26404827656252755, 2.9858224688390417e-06], 'SGDRegressor': [0.35036686833289443, 0.010050606676439778]}\n",
      "dict_errors_1_3 {'GradBoostRegressor': [64.05887104917701, 6.879228516611121], 'SVR': [57.472483085933554, 7.290719983843275], 'lienar_regression': [56.280924972230636, 5.710390790669493], 'neural_network': [45.78915480157528, 35.03483792790064], 'neer_neighboor': [54.907603756437446, 7.081187518933646], 'SGDRegressor': [56.52327577501767, 7.01050186812077]}\n",
      "dict_errors, after changes :  {'lienar_regression': [0.34321702049967723, 0.017421293770233204], 'neural_network': [0.26404827656252755, 2.9858224688390417e-06], 'SGDRegressor': [0.35036686833289443, 0.010050606676439778], 'GradBoostRegressor': [0.3059346708449163, 3.882566534691418]}\n",
      "dict_errors, after changes :  {'GradBoostRegressor': [64.05887104917701, 6.879228516611121], 'SVR': [57.472483085933554, 7.290719983843275], 'lienar_regression': [56.280924972230636, 5.710390790669493], 'neural_network': [45.78915480157528, 35.03483792790064], 'neer_neighboor': [54.907603756437446, 7.081187518933646], 'SGDRegressor': [56.52327577501767, 7.01050186812077]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a5b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
