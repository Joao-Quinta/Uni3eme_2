{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 Naive Bayes (OBLIGATORY)\n",
    "**individual work, deadline 29/03/2021 23:59**\n",
    "\n",
    "In this TP you are going to implement the Naive Bayes (NB) algorithm for categorical (titanic) and continuous (iris) data using **Python 3**.\n",
    "\n",
    "You are going to fill a few missing functions in the python script nb.py\n",
    "to implement the exercises that we ask. So first of all read and understand the given python script. To run your code you have to run the\n",
    "main TP1\\_NB.ipynb notebook. Here you have to write\n",
    "only a short code (it is mentioned where) to run the NB algorithm. Parts of the code are given and it works if the missing functions in nb.py are correctly implemented.\n",
    "\n",
    "For the categorical data you have to make the necessary modifications in the train_nb() and predict() functions in nb.py in order to work for both continuous and categorical data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import the iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_IRIS(test=True):\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = shuffle(iris.data, iris.target, random_state= 1230)\t\n",
    "    if test:\n",
    "        X_train = X[:100, :]\n",
    "        y_train = y[:100]\n",
    "        X_test = X[100:, :]\n",
    "        y_test = y[100:]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    else:\n",
    "        X = iris.data[:, :] \n",
    "        y = iris.target\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the iris data set\n",
    "X_train, y_train, X_test, y_test = load_IRIS(test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (100, 4)\n",
      "Training labels shape:  (100,)\n",
      "Test data shape:  (50, 4)\n",
      "Test labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 0, 2, 2, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 2, 0, 1, 1, 2,\n",
       "       2, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing parts in nb.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb import train_nb, normal_distribution, predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure that all the function work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)\n",
    "#print(normal_distribution(X_train, 0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior:  [0.32, 0.41, 0.27]\n",
      "mean:  [[5.01875    3.475      1.496875   0.25625   ]\n",
      " [5.94878049 2.74634146 4.23658537 1.30243902]\n",
      " [6.48518519 2.94074074 5.46666667 1.95925926]]\n",
      "std:  [[0.10902344 0.109375   0.03717773 0.01433594]\n",
      " [0.24737656 0.08785247 0.22573468 0.0348721 ]\n",
      " [0.43533608 0.11871056 0.28444444 0.06389575]]\n",
      "mean shape:  (3, 4)\n",
      "std shape:  (3, 4)\n"
     ]
    }
   ],
   "source": [
    "prior, mean, std = train_nb(X_train, y_train)\n",
    "print('Prior: ', prior)\n",
    "print('mean: ', mean)\n",
    "print('std: ', std)\n",
    "print('mean shape: ', mean.shape)\n",
    "print('std shape: ', std.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  [0, 1, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 0, 1, 1, 2, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 2, 1, 2, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 2, 1, 2, 2, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 2, 1, 2, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_train, prior, mean, std)\n",
    "print('Predicted label: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model and compute train and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prior, mean, std = train_nb(X_train, y_train)\n",
    "teste = [True, True, False]\n",
    "count = np.count_nonzero()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the train accuracy:\n",
    "y_train_pred = predict(X_train, prior, mean, std)\n",
    "num_correct_train = np.where()\n",
    "accuracy_train = \n",
    "print('Train: Got %d / %d correct => accuracy: %f' % (num_correct_train, len(y_train), accuracy_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the test accuracy:\n",
    "y_test_pred = \n",
    "num_correct_test = \n",
    "accuracy_test = (\n",
    "print('Test: Got %d / %d correct => accuracy: %f' % (num_correct_test, len(y_test), accuracy_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "## Decision surface\n",
    "Visualize, study, and **discuss the decision surfaces that NB algorithm produces for the iris data set**.\n",
    "\n",
    "\n",
    "* To do so, you will work only in two attributes(training and testing).\n",
    "\n",
    "     * Testing will be done on an artificially generated dataset that covers in a regular manner all possible values for the two chosen attributes. To do so we need to divide the space into a grid by discretizing the space  into $n$ values between the minimum and maximum value of an attribute. Each of these values must be compared with the $n$ discrete values of the second attribute. The resulting array will be of shape ($n$ * $n$, 2)\n",
    "     \n",
    "     * Using your training set classify your test instances and visualize the results of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris data.\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issues)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train = load_IRIS(test=False)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "# we use all the data as training\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_labels = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "# For our visualisation we will only keep the two first attributs.\n",
    "# You can choose 2 attribus here\n",
    "attribut_1 = 1\n",
    "attribut_2 = 3\n",
    "\n",
    "X_train_2_features = np.array([X_train[:,attribut_1], X_train[:,attribut_2]]).T\n",
    "axes_label_1 = axes_labels[attribut_1]\n",
    "axes_label_2 = axes_labels[attribut_2]\n",
    "\n",
    "# and the test set will be the matrix (n*n, 2) where n is the number of values taken between\n",
    "# the min and the max of each attribute\n",
    "n = 150\n",
    "\n",
    "X_test = np.zeros((n*n, 2))\n",
    "tmp = np.linspace(np.min(X_train_2_features[:, 0])-0.1, np.max(X_train_2_features[:, 0])+0.1, n)\n",
    "X_test[:,0] = np.repeat(tmp, n)\n",
    "\n",
    "tmp = np.linspace(np.min(X_train_2_features[:, 1])-0.1, np.max(X_train_2_features[:, 1])+0.1, n)\n",
    "X_test[:,1] = np.tile(tmp, n)\n",
    "\n",
    "\n",
    "print('Training data shape: ', X_train_2_features.shape)\n",
    "print('Testing data shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "from nb import train_nb, normal_distribution, predict\n",
    "\n",
    "\n",
    "prior, mean, std = train_nb(X_train_2_features, y_train)\n",
    "\n",
    "# test\n",
    "y_test_pred = predict(X_test, log_prior, mean_std)\n",
    "\n",
    "print('Predicted data shape : ', y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision surface\n",
    "colors_surfaces = ['#88E2EA', '#FFE1BB', '#D4F3CD']\n",
    "colors_points = ['blue', 'red', 'green']\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 16.0)\n",
    "plt.clf()\n",
    "fig = plt.figure() \n",
    "\n",
    "for n_class in range(3):\n",
    "    x_train_attri_1 = X_train_2_features[y_train == n_class, 0]\n",
    "    x_train_attri_2 = X_train_2_features[y_train == n_class, 1]\n",
    "        \n",
    "    x_test_attri_1 = X_test[y_test_pred == n_class, 0]\n",
    "    x_test_attri_2 = X_test[y_test_pred == n_class, 1]\n",
    "        \n",
    "    ax = fig.add_subplot(2,3,1+i)  \n",
    "    ax.set_title(\"K = \")\n",
    "    ax.scatter(x_test_attri_1, x_test_attri_2, s=15, color=colors_surfaces[n_class])\n",
    "    ax.scatter(x_train_attri_1, x_train_attri_2, s=15, color=colors_points[n_class])\n",
    "    ax.set_xlabel(axes_label_1)\n",
    "    ax.set_ylabel(axes_label_2)\n",
    "\n",
    "        \n",
    "# each color represent a class of the problem\n",
    "\n",
    "# don't take care of the warning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB for catedorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titanic.csv', newline='') as csvfile:\n",
    "        data = list(csv.reader(csvfile))\n",
    " \n",
    "#We suppose that the first line is an indication on the meaning of each\n",
    "#attribute so we can remove it.\n",
    "del data[0]\n",
    " \n",
    "#Randomizing the data (we use numpy)\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "data.tolist()\n",
    " \n",
    "#Splitting the set (2/3 for the training set / 1/3 for the test set)\n",
    "split_value = int(len(data) / 3)\n",
    "X_data = data[:,0:3]\n",
    "y_data = data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check, we print out the size of the data and the first line.\n",
    "\n",
    "print('Data shape:', data.shape)\n",
    "print('X_data shape:', X_data.shape)\n",
    "print('y_data shape:', y_data.shape)\n",
    "print('Data 1st line:', data[0])\n",
    "print('X_data 1st line:', X_data[0])\n",
    "print('y_data 1st line:', y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_data[:2*split_value]\n",
    "y_train = y_data[:2*split_value]\n",
    "\n",
    "X_test = X_data[2*split_value:]\n",
    "y_test = y_data[2*split_value:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a sanity check, we print out the size of the training and test data and the first line.\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "print('X_train 1st line:', X_train[0])\n",
    "print('y_train 1st line:', y_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB for categorical data\n",
    "\n",
    "- Implement the NB algorithm for categorical data. **Make the necessary modifications in train_nb() and predict() functions in order to work for both continuous and categorical data**.\n",
    " \n",
    "- Compute the classification accuracy of the titanic data set (test data) and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont forget to import your functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
